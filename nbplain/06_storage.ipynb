{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "funded-liberal",
   "metadata": {},
   "source": [
    "# Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-startup",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "- **Teaching:** 10 min\n",
    "- **Exercises:** 2 min\n",
    "\n",
    "**Questions**\n",
    "- What storage is available on Nimbus?\n",
    "- How does this affect my workflow?\n",
    "- Where should I keep my data?\n",
    "\n",
    "\n",
    "**Objectives**\n",
    "- Understand the storage structure of the new system.\n",
    "- Know where to keep your data, and how to stage data for runs.\n",
    "- Understand the costs associated with storage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-estonia",
   "metadata": {},
   "source": [
    "## Filesystem on Nimbus\n",
    "\n",
    "The filesystem on Nimbus, the University of Bath's Cloud HPC facility, has the following layout:\n",
    "\n",
    "![storage](../images/storage.png)\n",
    "\n",
    "The areas of the filesystem you should know about are:\n",
    "\n",
    "\n",
    "* ```/shared/home```: 2TB Azure Managed Disk. Each user is given a home folder, with a strict quota of 5GB. You can check your quota usage with ```quota -s```. This area can be accessed from both the login and compute nodes. This disk is backed up daily, and could be used for storing e.g.:\n",
    "    - Code/scripts for your calculations\n",
    "    - Template jobscripts\n",
    "\n",
    "* ```/campaign/```: each resource allocation has an associated campaign storage area located at ```/campaign/<resource_allocation_id>```. The quota for this disk space is set to 50GB for each account (account admins can allocate this to each Resource Allocation in the research computing account management portal). ```campaign``` is where you would keep files required for a current project thrust. This is not designed for long-term archival storage, and data should regularly be moved out of this area. Campaign storage is a resizable logical file system based on logical volume management (LVM). The file systems are created in multiple managed disks. Initially we have one 16TB disk, which can be increased to 64TB, with the ability to add more disks. \n",
    "\n",
    "* ```/apps/```: 1TB Azure managed Disk. This space contains centrally compiled software for use of users.\n",
    "\n",
    "* ```/u/```: The University `H` drive is mounted and can be accessed from the login nodes only.\n",
    "\n",
    "* ```/mnt/resource/```: Some compute instances have fast local storage located in this area, only accessible from the compute node. e.g. HBv3 (450GB), HBv2(450GB), HB(450GB), HC44(650GB) \n",
    "\n",
    "* ```/burstbuffer/*/```: when running a job on a compute node you can access one of 26 blob stores in `/burstbuffer/*/`. An environment variable `$BURSTBUFFER` is made available in the environment when running a job, and the job specific burstbuffer gets destroyed at the end of the job. You can use this space to store temporary files that are needed during a job run, bu not afterwards. e.g. by staging your input data in `$BURSTBUFFER`, running your job there, and then only copying the outputs you require back to either `/campaign` or `/home/`. The blob performance is 60 GBps shared by jobs running on each blob store (allocated in a round robin fashion)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-marathon",
   "metadata": {},
   "source": [
    "### Environment Variables\n",
    "\n",
    "There are several environment variables to help you navigate the filesystem.\n",
    "\n",
    "*```$HOME``` = /shared/home/`<username>`: your home directory\n",
    "\n",
    "*```$BUCSHOME``` = /u/u/`<username>`: your University `H` folder\n",
    "\n",
    "*```$BURSTBUFFER``` = /burstbuffer/[a-z]/`<slurm_job_id>`: burst buffer blob store for use during job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-calendar",
   "metadata": {},
   "source": [
    "## Exercise: Try these commands\n",
    "\n",
    "```bash\n",
    "echo $HOME\n",
    "echo $BUCSHOME\n",
    "cd $BUCSHOME\n",
    "pwd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-principle",
   "metadata": {},
   "source": [
    "## Exercise: Go home\n",
    "\n",
    "How can we use the environment variables we've used above to return to our `home` directory?\n",
    "\n",
    "[Solution]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-directive",
   "metadata": {},
   "source": [
    "## Solution: Go home\n",
    "\n",
    "We can use the `$HOME` variable in exactly the same way as we used `$SCRATCH`:\n",
    "\n",
    "```bash\n",
    "cd $HOME\n",
    "pwd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f96a1",
   "metadata": {},
   "source": [
    "## Storage Costs\n",
    "\n",
    "Currently storage will not be charged, until a low cost tier three storage option is in place. At which point campaign storage will be charged, and account administrators will be able to allocate campaign storage, see usage and storage transactions in the research computing account management portal.\n",
    "\n",
    "Although, we would encourage users to get into the habit of moving data around as a fundamental part of their workflow in order to optimise their own costs, and make most efficient use of the storage options available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-claim",
   "metadata": {},
   "source": [
    "## Key Points:\n",
    "- There are different storage areas on the Nimbus system.\n",
    "- **Only** your `/home/` area is backed up.\n",
    "- Data required for your current project workflow should be kept in `/campaign/`.\n",
    "- Data should not be stored on `$BURSTBUFFER` is an environment variable pointing to temporary storage that can be used throughout the duration of your job. \n",
    "- You can use environment variables to navigate the filesystem.\n",
    "- Storage is currently not being charged, but will be in the future, so getting into good habits with respect to data management (and encouraging others to do the same) is certainly a good idea.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
