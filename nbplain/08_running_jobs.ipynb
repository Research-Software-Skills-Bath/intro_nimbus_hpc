{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bridal-option",
   "metadata": {},
   "source": [
    "# Running a job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-designer",
   "metadata": {},
   "source": [
    "Below are example submission scripts for several different codes.\n",
    "\n",
    "These examples copy the run data from the submission directory in `/campaign/` area to `/mnt/resource/` before the run, and copy any files back to `/campaign/` afterwards. The script then cleans up by removing the `working_directory` in the `$BURSTBUFFER` area (this will be done automatically, but is perhaps a good habit to get into for furture developments in terms of storage were this may not be the case).\n",
    "\n",
    "\n",
    "Obviously there are an infinite number of ways to structure the logic in your submission scripts in order to handle the data transfer between `/campaign/` and `/mnt/resource`.\n",
    "\n",
    "The following examples can be adapted to your own needs. They should be submitted from the `/campaign/` storage area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2c018",
   "metadata": {},
   "source": [
    "## Slurm directives\n",
    "\n",
    "Every line that starts with a `#SBATCH` is a directive for `slurm` this is where we tell slurm the resources we want for our job. \n",
    "\n",
    "`#SBATCH --account=` tells slurm what account you wish to run against. The account code used in your run script `#SBATCH --account=ACCOUNT_CODE` should match the resource allocation you wish to run your job against. If you don't know your resource allocation code check the research computing account managemnet portal at `rcam.bath.ac.uk` (you need to be on the University's VPN All traffic), or ask your account administrator. or use the command `sacctmgr show associations user=userid --parsable2`. This command will also tell you the limits on the account, and what QOS (and partitions) you have access to. \n",
    "\n",
    "`#SBATCH --job-name=` gives the job a name, which you can identify in the queue - you can call the job whatever you want that helps you to identify it. \n",
    "\n",
    "`#SBATCH --partition=` tells slurm what partition to put the job on. \n",
    "\n",
    "`#SBATCH --qos=` tells slurm what Quality of Service you wish to run with. The QOS is simply a way for admins to apply rules to the resources you can access, and the priority of the job. For Nimbus HPC systems the qos will match the partition name - and **remember** if you don't include it you will get an error.\n",
    "\n",
    "`#SBATCH --time=` tells slurm how long you wish to run the job for, with several acceptable formats:  \"minutes\", \"minutes:seconds\", \"hours:minutes:seconds\", \"days-hours\", \"days-hours:minutes\" and \"days-hours:minutes:seconds\" e.g. `1-01:20:00` will request a runtime of 25 hours and 20 mins. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563cf6c1",
   "metadata": {},
   "source": [
    "## Exercise: submitting a job\n",
    "\n",
    "First create a text file named `test.txt` with the following in your campaign folder. \n",
    "\n",
    "```bash\n",
    "My first nimbus run\n",
    "```\n",
    "\n",
    "Now create a runscript to copy this file over to the fast local storage at `/mnt/resource/`, print the details to an output file called `my_output.txt` and copy the output back at the end of the run.\n",
    "\n",
    "Hint:  You can check what accounts you have access to with `sacctmgr`, and you will also have access to a folder /campaign/account_code (not case sensitive - the campaign folder will be in capitals).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b26375",
   "metadata": {},
   "source": [
    "##Â Solution: submitting a job\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --account=account_code_here\n",
    "#SBATCH --job-name=JOB_NAME\n",
    "#SBATCH --output=%x.%j.o\n",
    "#SBATCH --error=%x.%j.e\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --partition=spot-fsv2-1\n",
    "#SBATCH --qos=spot-fsv2-1\n",
    "#SBATCH --time=04:00:00\n",
    "\n",
    "# set campaigndir as our current working directory for copy back\n",
    "campaigndir=$(pwd)\n",
    "\n",
    "cp test.txt /mnt/resource\n",
    "cat test.txt > my_output.txt\n",
    "cp my_output.txt $campaigndir/\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6c6d2f",
   "metadata": {},
   "source": [
    "## Information: Copying back the stderr and stdout files\n",
    "\n",
    "If we copy everythig in our working directory to the compute node we will also copy the `stderr` and `stdout` files given by:\n",
    "\n",
    "```bash\n",
    "#SBATCH --output=%x.%j.o\n",
    "#SBATCH --error=%x.%j.e\n",
    "```\n",
    "\n",
    "This will copy them at a snapshot in time from the start of our job. The file will continue to be written to in our campaign directory - so either we must specify a `stderr` `stdout` file on the local storage, or **be sure not to copy back the `stderr` and `stdout` files from `/mnt/resource/`** - as it will over write anything that has subsequently been written to them with the files from an earlier point in time. \n",
    "\n",
    "The next example runscript shows an approach for using rsync to avoid copying these files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-benefit",
   "metadata": {},
   "source": [
    "## Openfoam\n",
    "\n",
    "We will run an example job using the OpenFoam module for the hbv3-120 instance\n",
    "\n",
    "`OpenFOAM/v2012-foss-2020a`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-darwin",
   "metadata": {},
   "source": [
    "An example run script - create a file named `run_job.slm` with the following contents (remembering to replace the account code with that of your resource allocation):\n",
    "    \n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --account=prj3_phase1\n",
    "#SBATCH --job-name=JOB_NAME\n",
    "#SBATCH --output=JOB_NAME.%j.o\n",
    "#SBATCH --error=JOB_NAME.%j.e\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=120\n",
    "#SBATCH --partition=spot-hbv3-120\n",
    "#SBATCH --qos=spot-hbv3-120\n",
    "#SBATCH --time=04:00:00\n",
    "\n",
    "\n",
    "# Load the openFOAM module \n",
    "module purge\n",
    "module load OpenFOAM/v2012-foss-2020a\n",
    "\n",
    "# as en example we will copy the dambreak tutorial to our current\n",
    "# directory\n",
    "cp -r $WM_PROJECT_DIR/tutorials/multiphase/interFoam/laminar/damBreak ./\n",
    "cd damBreak\n",
    "\n",
    "# set campaigndir as our current working directory for copy back\n",
    "\n",
    "campaigndir=$(pwd)\n",
    "\n",
    "# Check we are submitting from campaign \n",
    "echo $campaigndir | if grep -q campaign; then \n",
    "    # Copy any inputs required to the work directory:\n",
    "    # excluding and files with a <JOB_NAME> (whatever your job name actualy is...) prefix, so your\n",
    "    # output and error files don't get over written on the copy back\n",
    "    rsync -aP --exclude=JOB_NAME.* $campaigndir/* $BURSTBUFFER\n",
    "\n",
    "    cd $BURSTBUFFER;\n",
    "    echo  \"Work directory\" $BURSTBUFFER ;\n",
    "    \n",
    "    # source the foamDotFile and do the run\n",
    "    source $WM_PROJECT_DIR/etc/bashrc\n",
    "    ./Allrun\n",
    "\n",
    "    # Copy back any results you need to campaign.\n",
    "    srun cp -Rf $BURSTBUFFER/*  $campaigndir/\n",
    "    #Clean up burstbuffer\n",
    "    rm -rf $BURSTBUFFER\n",
    "else\n",
    "     echo \"Not submitting from campaign. Check your submission script.\"\n",
    "     exit 1;\n",
    "fi\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-lying",
   "metadata": {},
   "source": [
    "\n",
    "And to run the job issue the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "sbatch run_job.slm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-delivery",
   "metadata": {},
   "source": [
    "### Interactive job\n",
    "\n",
    "It is also possible to obtain a compute node and work interactively. \n",
    "\n",
    "Issuing the following command in the terminal will request a spot-hbv3-120 compute instance for 6 hours:\n",
    "\n",
    "```\n",
    "srun --partition spot-hbv3-120 --nodes 1  --account prj4_phase1  --qos spot-hbv3-120 --job-name \"interactive\" --cpus-per-task 120 --time 6:00:00 --pty bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e093085",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
